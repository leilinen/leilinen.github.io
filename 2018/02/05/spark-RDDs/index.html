<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="spark 算子总结"><meta name="keywords" content="spark"><meta name="author" content="leiline"><meta name="copyright" content="leiline"><title>spark 算子总结 | 李林林的小站</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#spark算子分类"><span class="toc-number">1.</span> <span class="toc-text">spark算子分类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformation算子"><span class="toc-number">2.</span> <span class="toc-text">transformation算子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#value类型transformation算子"><span class="toc-number">2.1.</span> <span class="toc-text">value类型transformation算子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#map算子"><span class="toc-number">2.1.1.</span> <span class="toc-text">map算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#flatMap算子"><span class="toc-number">2.1.2.</span> <span class="toc-text">flatMap算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapPartitions算子"><span class="toc-number">2.1.3.</span> <span class="toc-text">mapPartitions算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#glom-算子"><span class="toc-number">2.1.4.</span> <span class="toc-text">glom 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#union算子"><span class="toc-number">2.1.5.</span> <span class="toc-text">union算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cartesian算子"><span class="toc-number">2.1.6.</span> <span class="toc-text">cartesian算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#groupBy算子"><span class="toc-number">2.1.7.</span> <span class="toc-text">groupBy算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#filter算子"><span class="toc-number">2.1.8.</span> <span class="toc-text">filter算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#distinct算子"><span class="toc-number">2.1.9.</span> <span class="toc-text">distinct算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#subtract算子"><span class="toc-number">2.1.10.</span> <span class="toc-text">subtract算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sample算子"><span class="toc-number">2.1.11.</span> <span class="toc-text">sample算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#takeSample算子"><span class="toc-number">2.1.12.</span> <span class="toc-text">takeSample算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cache算子"><span class="toc-number">2.1.13.</span> <span class="toc-text">cache算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#persist算子"><span class="toc-number">2.1.14.</span> <span class="toc-text">persist算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapValues算子"><span class="toc-number">2.1.15.</span> <span class="toc-text">mapValues算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#combineByKey算子"><span class="toc-number">2.1.16.</span> <span class="toc-text">combineByKey算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reduceByKey算子"><span class="toc-number">2.1.17.</span> <span class="toc-text">reduceByKey算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#partitionBy算子"><span class="toc-number">2.1.18.</span> <span class="toc-text">partitionBy算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cogroup算子"><span class="toc-number">2.1.19.</span> <span class="toc-text">Cogroup算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#join算子"><span class="toc-number">2.1.20.</span> <span class="toc-text">join算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#leftOutJoin和rightOutJoin算子"><span class="toc-number">2.1.21.</span> <span class="toc-text">leftOutJoin和rightOutJoin算子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#action类型算子"><span class="toc-number">2.2.</span> <span class="toc-text">action类型算子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#foreach算子"><span class="toc-number">2.2.1.</span> <span class="toc-text">foreach算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsTextFile算子"><span class="toc-number">2.2.2.</span> <span class="toc-text">saveAsTextFile算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsObjectFile算子"><span class="toc-number">2.2.3.</span> <span class="toc-text">saveAsObjectFile算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#collect算子"><span class="toc-number">2.2.4.</span> <span class="toc-text">collect算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#collectAsMap算子"><span class="toc-number">2.2.5.</span> <span class="toc-text">collectAsMap算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reduceByKeyLocally算子"><span class="toc-number">2.2.6.</span> <span class="toc-text">reduceByKeyLocally算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loopup算子"><span class="toc-number">2.2.7.</span> <span class="toc-text">loopup算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#count算子"><span class="toc-number">2.2.8.</span> <span class="toc-text">count算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#top算子"><span class="toc-number">2.2.9.</span> <span class="toc-text">top算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reduce算子"><span class="toc-number">2.2.10.</span> <span class="toc-text">reduce算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fold算子"><span class="toc-number">2.2.11.</span> <span class="toc-text">fold算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#aggregate算子"><span class="toc-number">2.2.12.</span> <span class="toc-text">aggregate算子</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">leiline</div><div class="author-info__description text-center">知道你要去很远的地方，但请一定记得回头看看</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">35</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">16</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">3</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">李林林的小站</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">首页</a><a class="site-page" href="/categories/technology">技艺</a><a class="site-page" href="/categories/life">生活</a><a class="site-page" href="/categories/others">其他</a><a class="site-page" href="/about">关于</a><a class="site-page" href="/archives">归档</a></span></div><div id="post-info"><div id="post-title">spark 算子总结</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-02-05</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/technology/">technology</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>本文主要介绍spark中常用的算子的作用记忆部分示例总结。</p>
<a id="more"></a>

<h1 id="spark算子分类"><a href="#spark算子分类" class="headerlink" title="spark算子分类"></a>spark算子分类</h1><p>从总体上来分，spark算子可以分为两大类：transformation和action，其中transformation触发会记录元数据信息，延迟执行，只有触发action才会真正的执行计算。</p>
<p>从小方向上来讲，spark算子可以分为三种类型：value类型transformation算子，key-value类型transformation算子以及action算子。</p>
<blockquote>
<p>what is RDD?<br>Internally, each RDD is characterized by five main properties:</p>
<ul>
<li>A list of partitions</li>
<li>A function for computing each split</li>
<li>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partioned)</li>
<li>Optionally, a list of preferred locations to compute each split on(e.g. block locations for an HDFS file)</li>
</ul>
</blockquote>
<p>接下来介绍常用的一些spark算子。</p>
<blockquote>
<p>最近发现了一个很好的对spark rdd进行解释的页面，很多文档中没有的rdd也有解释。<br>链接： <a href="http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html" target="_blank" rel="noopener">http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html</a></p>
</blockquote>
<h1 id="transformation算子"><a href="#transformation算子" class="headerlink" title="transformation算子"></a>transformation算子</h1><h2 id="value类型transformation算子"><a href="#value类型transformation算子" class="headerlink" title="value类型transformation算子"></a>value类型transformation算子</h2><h3 id="map算子"><a href="#map算子" class="headerlink" title="map算子"></a>map算子</h3><p>map算子将原来RDD中的每一个数据通过map中的用户自定义方法f映射转变成新的元素。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.parallelize(List(1, 2, 3))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = rdd1.map(_*10) </span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.collect()</span><br><span class="line">res0: Array[Int] = Array(10, 20, 30)</span><br></pre></td></tr></table></figure>

<h3 id="flatMap算子"><a href="#flatMap算子" class="headerlink" title="flatMap算子"></a>flatMap算子</h3><p>flatMap将RDD中的每一个元素通过函数f转成新的元素，并将RDD的每个集合中的元素合并成一个集合。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.parallelize(Array(&quot;a b c&quot;, &quot;d e f&quot;, &quot;h i j&quot;))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = rdd1.flatMap(_.split(&apos; &apos;)).collect</span><br><span class="line">rdd2: Array[String] = Array(a, b, c, d, e, f, h, i, j)</span><br></pre></td></tr></table></figure>

<h3 id="mapPartitions算子"><a href="#mapPartitions算子" class="headerlink" title="mapPartitions算子"></a>mapPartitions算子</h3><p>与map类似，但是在RDD的每个分区（块）上分别运行，所以当在T类型的RDD上运行时，func的类型必须是Iterator <t> =&gt; Iterator <u>。</u></t></p>
<h3 id="glom-算子"><a href="#glom-算子" class="headerlink" title="glom 算子"></a>glom 算子</h3><p>glom将每个分区形成一个数组，内部实现返回ClommedRDD</p>
<h3 id="union算子"><a href="#union算子" class="headerlink" title="union算子"></a>union算子</h3><p>使用union函数要保证两个RDD元素的数据类型相同，返回的RDD类型与被合并的RDD数据类型相同，并不进行去重操作，保存所有元素。</p>
<p>如果去重，可以使用distinct()进行处理。</p>
<h3 id="cartesian算子"><a href="#cartesian算子" class="headerlink" title="cartesian算子"></a>cartesian算子</h3><p>对两个RDD的所有元素进行笛卡尔积操作，返回CartesianRDD.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.parallelize(List(&quot;tom&quot;, &quot;jerry&quot;))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[21] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = sc.parallelize(List(&quot;tom&quot;, &quot;kitty&quot;, &quot;shuke&quot;))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[22] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd3 = rdd1.cartesian(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.RDD[(String, String)] = CartesianRDD[23] at cartesian at &lt;console&gt;:28</span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect</span><br><span class="line">res3: Array[(String, String)] = Array((tom,tom), (tom,kitty), (tom,shuke), (jerry,tom), (jerry,kitty), (jerry,shuke))</span><br></pre></td></tr></table></figure>

<h3 id="groupBy算子"><a href="#groupBy算子" class="headerlink" title="groupBy算子"></a>groupBy算子</h3><p>将元素通过函数生成相应的key，数据转换为key-value形式，将key相同的元素分为一组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd3.collect</span><br><span class="line">res1: Array[(String, (Int, Int))] = Array((tom,(1,8)), (jerry,(2,9)))</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd4 = rdd3.groupByKey</span><br><span class="line">rdd4: org.apache.spark.rdd.RDD[(String, Iterable[(Int, Int)])] = MapPartitionsRDD[20] at groupByKey at &lt;console&gt;:30</span><br><span class="line"></span><br><span class="line">scala&gt; rdd4.collect</span><br><span class="line">res2: Array[(String, Iterable[(Int, Int)])] = Array((tom,CompactBuffer((1,8))), (jerry,CompactBuffer((2,9))))</span><br></pre></td></tr></table></figure>

<h3 id="filter算子"><a href="#filter算子" class="headerlink" title="filter算子"></a>filter算子</h3><p>对RDD中的元素按照函数f进行过滤。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(1 to 9, 2)</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd = a.filter(_ &lt;5).collect</span><br><span class="line">rdd: Array[Int] = Array(1, 2, 3, 4)</span><br></pre></td></tr></table></figure>

<h3 id="distinct算子"><a href="#distinct算子" class="headerlink" title="distinct算子"></a>distinct算子</h3><p>对RDD中的元素进行去重操作。</p>
<h3 id="subtract算子"><a href="#subtract算子" class="headerlink" title="subtract算子"></a>subtract算子</h3><p>对两个RDD进行集合的差操作。</p>
<h3 id="sample算子"><a href="#sample算子" class="headerlink" title="sample算子"></a>sample算子</h3><p>sample 将 RDD 这个集合内的元素进行采样，获取所有元素的子集。用户可以设定是否有放回的抽样、百分比、随机种子，进而决定采样方式。内部实现是生成 SampledRDD(withReplacement， fraction， seed)。</p>
<h3 id="takeSample算子"><a href="#takeSample算子" class="headerlink" title="takeSample算子"></a>takeSample算子</h3><p>类似sample算子，通过设定采样个数进行采样，返回结果的集合为单机数组。</p>
<h3 id="cache算子"><a href="#cache算子" class="headerlink" title="cache算子"></a>cache算子</h3><p>将RDD元素缓存到内存中</p>
<h3 id="persist算子"><a href="#persist算子" class="headerlink" title="persist算子"></a>persist算子</h3><p>persist 函数对 RDD 进行缓存操作。</p>
<blockquote>
<p>cache() 与 persist()的区别</p>
<p>With cache(), you use only the default storage level MEMORY_ONLY. With persist(), you can specify which storage level you want,(rdd-persistence).</p>
<p>From the official docs:</p>
<p>You can mark an RDD to be persisted using the persist() or cache() methods on it.<br>each persisted RDD can be stored using a different storage level<br>The cache() method is a shorthand for using the default storage level, which is StorageLevel.MEMORY_ONLY (store deserialized objects in memory).<br>Use persist() if you want to assign another storage level than MEMORY_ONLY to the RDD (which storage level to choose)</p>
</blockquote>
<h3 id="mapValues算子"><a href="#mapValues算子" class="headerlink" title="mapValues算子"></a>mapValues算子</h3><p>针对(key, value)类型中的value进行map操作。</p>
<h3 id="combineByKey算子"><a href="#combineByKey算子" class="headerlink" title="combineByKey算子"></a>combineByKey算子</h3><p>按照键值key进行聚合。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val initialScores = Array((&quot;Fred&quot;, 88.0), (&quot;Fred&quot;, 95.0), (&quot;Fred&quot;, 91.0), (&quot;Wilma&quot;, 93.0), (&quot;Wilma&quot;, 95.0), (&quot;Wilma&quot;, 98.0))  </span><br><span class="line">val d1 = sc.parallelize(initialScores)  </span><br><span class="line">type MVType = (Int, Double) //定义一个元组类型(科目计数器,分数)  </span><br><span class="line">d1.combineByKey(  </span><br><span class="line">  score =&gt; (1, score),  </span><br><span class="line">  (c1: MVType, newScore) =&gt; (c1._1 + 1, c1._2 + newScore),  </span><br><span class="line">  (c1: MVType, c2: MVType) =&gt; (c1._1 + c2._1, c1._2 + c2._2)  </span><br><span class="line">).map &#123; case (name, (num, socre)) =&gt; (name, socre / num) &#125;.collect</span><br></pre></td></tr></table></figure>

<h3 id="reduceByKey算子"><a href="#reduceByKey算子" class="headerlink" title="reduceByKey算子"></a>reduceByKey算子</h3><p>combineByKey的简单版，将两个值按照key进行合并。</p>
<h3 id="partitionBy算子"><a href="#partitionBy算子" class="headerlink" title="partitionBy算子"></a>partitionBy算子</h3><p>将RDD进行分区操作。</p>
<h3 id="Cogroup算子"><a href="#Cogroup算子" class="headerlink" title="Cogroup算子"></a>Cogroup算子</h3><h3 id="join算子"><a href="#join算子" class="headerlink" title="join算子"></a>join算子</h3><p>join算子对两个需要连接的RDD进行cogroup操作，将相同key的数据放在一个分区，之后对新的RDD的每一个key元素进行笛卡尔积操作，最后构成一个新的集合。返回值为RDD[(K, (V, W))]</p>
<p><img src="images/855959-20160731205235169-1186414997.png" alt="join "></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.parallelize(List((&quot;tom&quot;, 1), (&quot;jerry&quot;, 2), (&quot;kitty&quot;, 3)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = sc.parallelize(List((&quot;jerry&quot;, 9), (&quot;tom&quot;, 8), (&quot;shuke&quot;, 7)))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd3 = rdd1.join(rdd2).collect</span><br><span class="line">rdd3: Array[(String, (Int, Int))] = Array((tom,(1,8)), (jerry,(2,9)))</span><br></pre></td></tr></table></figure>

<h3 id="leftOutJoin和rightOutJoin算子"><a href="#leftOutJoin和rightOutJoin算子" class="headerlink" title="leftOutJoin和rightOutJoin算子"></a>leftOutJoin和rightOutJoin算子</h3><p>LeftOutJoin（左外连接）和RightOutJoin（右外连接）相当于在join的基础上先判断一侧的RDD元素是否为空，如果为空，则填充为空。 如果不为空，则将数据进行连接运算，并返回结果。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd4 = rdd1.leftOuterJoin(rdd2).collect</span><br><span class="line">rdd4: Array[(String, (Int, Option[Int]))] = Array((tom,(1,Some(8))), (jerry,(2,Some(9))), (kitty,(3,None)))</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd5 = rdd1.rightOuterJoin(rdd2).collect</span><br><span class="line">rdd5: Array[(String, (Option[Int], Int))] = Array((tom,(Some(1),8)), (jerry,(Some(2),9)), (shuke,(None,7)))</span><br></pre></td></tr></table></figure>

<h2 id="action类型算子"><a href="#action类型算子" class="headerlink" title="action类型算子"></a>action类型算子</h2><h3 id="foreach算子"><a href="#foreach算子" class="headerlink" title="foreach算子"></a>foreach算子</h3><p>foreach对RDD中的每个元素都应用函数f，返回UNIT</p>
<h3 id="saveAsTextFile算子"><a href="#saveAsTextFile算子" class="headerlink" title="saveAsTextFile算子"></a>saveAsTextFile算子</h3><p>将数据输出存储在HDFS指定目录中。</p>
<h3 id="saveAsObjectFile算子"><a href="#saveAsObjectFile算子" class="headerlink" title="saveAsObjectFile算子"></a>saveAsObjectFile算子</h3><p>将数据输出以sequencefile的格式存储在HDFS目录中。</p>
<h3 id="collect算子"><a href="#collect算子" class="headerlink" title="collect算子"></a>collect算子</h3><p>collect将分布式的RDD返回一个ARRAY类型数组。</p>
<h3 id="collectAsMap算子"><a href="#collectAsMap算子" class="headerlink" title="collectAsMap算子"></a>collectAsMap算子</h3><p>对(k, v)类型RDD数据返回一个单机hashMap，对于重复的RDD元素，后面的元素覆盖前面的元素。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var rdd1 = sc.parallelize(List((&quot;A&quot;, 1), (&quot;B&quot;, 2), (&quot;C&quot;, 3)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = rdd1.collectAsMap()</span><br><span class="line">rdd2: scala.collection.Map[String,Int] = Map(A -&gt; 1, C -&gt; 3, B -&gt; 2)</span><br></pre></td></tr></table></figure>

<h3 id="reduceByKeyLocally算子"><a href="#reduceByKeyLocally算子" class="headerlink" title="reduceByKeyLocally算子"></a>reduceByKeyLocally算子</h3><p>实现的是先reduce再collectAsMap的功能，先对RDD的整体进行reduce操作，然后再收集所有结果返回为一个HashMap。</p>
<h3 id="loopup算子"><a href="#loopup算子" class="headerlink" title="loopup算子"></a>loopup算子</h3><p>作用于K-V类型的RDD上，返回指定K的所有V值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2 = rdd1.lookup(&quot;A&quot;)</span><br><span class="line">rdd2: Seq[Int] = WrappedArray(1)</span><br></pre></td></tr></table></figure>

<h3 id="count算子"><a href="#count算子" class="headerlink" title="count算子"></a>count算子</h3><p>返回RDD的元素个数。</p>
<h3 id="top算子"><a href="#top算子" class="headerlink" title="top算子"></a>top算子</h3><p>返回最大的K个元素。</p>
<h3 id="reduce算子"><a href="#reduce算子" class="headerlink" title="reduce算子"></a>reduce算子</h3><p>对RDD中的元素进行reduceLeft函数的操作。分别对每个分区进行聚合。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(1 to 100, 3)</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; a.reduce(_ + _)</span><br><span class="line">res0: Int = 5050</span><br></pre></td></tr></table></figure>

<h3 id="fold算子"><a href="#fold算子" class="headerlink" title="fold算子"></a>fold算子</h3><p>聚合每个分区的值，每个分区内的聚合变量用zeroValue初始化。</p>
<p><strong>def fold(zeroValue: T)(op: (T, T) =&gt; T): T</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(1 to 100, 3)</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; a.fold(0)(_ + _)</span><br><span class="line">res2: Int = 5050</span><br><span class="line"></span><br><span class="line">scala&gt; a.fold(1)(_ + _)</span><br><span class="line">res3: Int = 5054</span><br><span class="line"></span><br><span class="line">scala&gt; a.fold(2)(_ + _)</span><br><span class="line">res4: Int = 5058</span><br></pre></td></tr></table></figure>

<h3 id="aggregate算子"><a href="#aggregate算子" class="headerlink" title="aggregate算子"></a>aggregate算子</h3><p>aggregate先对每个分区的所有元素进行aggregate操作，再对分区的结果进行fold操作。</p>
<p>aggreagate与fold和reduce的不同之处在于，<strong>aggregate相当于采用归并的方式进行数据聚集，</strong>这种聚集是并行化的。 而在fold和reduce函数的运算过程中，每个分区中需要进行串行处理，每个分区串行计算完结果，结果再按之前的方式进行聚集，并返回最终聚集结果。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; a.aggregate(0)(_+_, _+_)</span><br><span class="line">res8: Int = 5050</span><br></pre></td></tr></table></figure>

<p>根据Zhen He的解释，聚合功能允许用户将两个不同的函数应用在RDD中，在每个分区中应用第一个reduce函数，将每个分区内的数据减少为单个结果。第二个reduce函数函数将用于将所有分区的结果聚合在一起，以得到最终结果。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">leiline</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/02/05/spark-RDDs/">http://yoursite.com/2018/02/05/spark-RDDs/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/spark/">spark</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/02/18/manage-money/"><i class="fa fa-chevron-left">  </i><span>小白理财入门</span></a></div><div class="next-post pull-right"><a href="/2018/01/31/set-bridge-mode-for-ubuntu/"><span>修改ubuntu16.04为桥接模式上网</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2019 By leiline</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>