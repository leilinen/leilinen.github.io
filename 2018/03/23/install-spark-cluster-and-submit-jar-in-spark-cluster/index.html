<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="安装spark集群以及提交jar包到集群中"><meta name="keywords" content="spark"><meta name="author" content="leiline"><meta name="copyright" content="leiline"><title>安装spark集群以及提交jar包到集群中 | 李林林的精神驿站</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#机器准备"><span class="toc-number">1.</span> <span class="toc-text">机器准备</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#机器环境配置"><span class="toc-number">2.</span> <span class="toc-text">机器环境配置</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#安装Java-jdk"><span class="toc-number">2.1.</span> <span class="toc-text">安装Java jdk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#配置ssh免密登录"><span class="toc-number">2.2.</span> <span class="toc-text">配置ssh免密登录</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#配置Spark集群"><span class="toc-number">3.</span> <span class="toc-text">配置Spark集群</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#向spark-集群中提交jar包"><span class="toc-number">4.</span> <span class="toc-text">向spark 集群中提交jar包</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#编写代码"><span class="toc-number">4.1.</span> <span class="toc-text">编写代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#提交jar包"><span class="toc-number">4.2.</span> <span class="toc-text">提交jar包</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">leiline</div><div class="author-info__description text-center">知道你要去很远的地方，但请一定记得回头看看</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">43</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">21</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">3</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://pic4.zhimg.com/80/v2-4ec42922163b93f676bba3a169225154_1440w.jpg?source=1940ef5c)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">李林林的精神驿站</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">首页</a><a class="site-page" href="/categories/technology">技艺</a><a class="site-page" href="/categories/life">生活</a><a class="site-page" href="/categories/others">其他</a><a class="site-page" href="/about">关于</a><a class="site-page" href="/archives">归档</a></span></div><div id="post-info"><div id="post-title">安装spark集群以及提交jar包到集群中</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-03-23</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/technology/">technology</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>本文介绍安装spark集群并如何提交jar到spark集群中。</p>
<a id="more"></a>

<p>spark集群的安装方式与hadoop集群的安装方式比较类似，关于hadoop集群安装教程可以左转<a href="http://www.powerxing.com/install-hadoop-cluster/" target="_blank" rel="noopener">Hadoop集群安装配置教程_Hadoop2.6.0_Ubuntu/CentOS</a></p>
<h1 id="机器准备"><a href="#机器准备" class="headerlink" title="机器准备"></a>机器准备</h1><p>首先需要准备几台服务器，我这里有3台，并保证服务器之间能够相互通信。网络配置也请参照<a href="http://www.powerxing.com/install-hadoop-cluster/" target="_blank" rel="noopener">Hadoop集群安装配置教程_Hadoop2.6.0_Ubuntu/CentOS</a></p>
<p>其中，主服务器master，两个slave服务器：node1, node3</p>
<h1 id="机器环境配置"><a href="#机器环境配置" class="headerlink" title="机器环境配置"></a>机器环境配置</h1><p>接下来需要对机器环境进行配置。</p>
<h2 id="安装Java-jdk"><a href="#安装Java-jdk" class="headerlink" title="安装Java jdk"></a>安装Java jdk</h2><p>首先安装java jdk8到每一台服务器中，我这里将下载的安装包解压到/usr/local/java目录中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">leiline@master:/usr/local/java$ ls</span><br><span class="line">jdk1.8.0_131</span><br></pre></td></tr></table></figure>

<p>并配置java jdk路径到系统路径中。打开.bashrc文件，并写入</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">leiline@master:/usr/local/java$ vim ~/.bashrc</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/java/jdk1.8.0_131</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib</span><br></pre></td></tr></table></figure>

<h2 id="配置ssh免密登录"><a href="#配置ssh免密登录" class="headerlink" title="配置ssh免密登录"></a>配置ssh免密登录</h2><p>Spark集群之间是master通过ssh登录和slave进行通信的，因此需要配置ssh免密登录。ssh免密登录的原理就是将master服务器的公钥加入到加入到slave的~/.ssh/authorized_keys中。</p>
<p>首先要生成秘钥</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ~/.ssh               # 如果没有该目录，先执行一次ssh localhost</span><br><span class="line">rm ./id_rsa*            # 删除之前生成的公匙（如果有）</span><br><span class="line">ssh-keygen -t rsa       # 一直按回车就可以</span><br></pre></td></tr></table></figure>

<p>将公钥内容加入到authorized_keys中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ./id_rsa.pub &gt;&gt; ./authorized_keys</span><br></pre></td></tr></table></figure>

<p>最后将authorized_keys发送到slave中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp ~/.ssh/id_rsa.pub node1@Slave1:~/.ssh/</span><br></pre></td></tr></table></figure>

<p>尝试在master使用ssh登录到node1上:</p>
<figure class="highlight plain"><figcaption><span>ssh node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Welcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.13.0-37-generic x86_64)</span><br><span class="line"></span><br><span class="line"> * Documentation:  https://help.ubuntu.com</span><br><span class="line"> * Management:     https://landscape.canonical.com</span><br><span class="line"> * Support:        https://ubuntu.com/advantage</span><br><span class="line"></span><br><span class="line">263 packages can be updated.</span><br><span class="line">3 updates are security updates.</span><br><span class="line"></span><br><span class="line">Last login: Fri Mar 23 11:19:08 2018 from 122.204.142.185</span><br><span class="line">leiline@node1:~$</span><br></pre></td></tr></table></figure>

<p>如果没有遇到输入密码的提醒，就表示ssh免密登录配置成功了。</p>
<h1 id="配置Spark集群"><a href="#配置Spark集群" class="headerlink" title="配置Spark集群"></a>配置Spark集群</h1><ul>
<li><p>下载spark安装包，解压到目录中/usr/local/spark</p>
</li>
<li><p>修改/conf/spark-env.sh.template 为spark-env.sh</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv conf/spark-env.template conf/spark-env.sh</span><br></pre></td></tr></table></figure>


<ul>
<li>修改spark-env.sh文件,向文件中添加内容</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/hadoop-2.7.3/bin/hadoop classpath)</span><br><span class="line">export JAVA_HOME=/usr/local/java/jdk1.8.0_131</span><br><span class="line">export SPARK_MASTER_IP=master</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure>

<ul>
<li>修改slaves.template文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv slaves.template slaves</span><br></pre></td></tr></table></figure>


<p>并向slaves中添加node1, node3： </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># A Spark Worker will be started on each of the machines listed below.</span><br><span class="line">node1</span><br><span class="line">node3</span><br></pre></td></tr></table></figure>


<ul>
<li>在node1和node3中创建spark目录，并修改spark目录的权限</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">leiline@node3:/usr/local$ sudo mkdir spark</span><br><span class="line">leiline@node3:/usr/local$ sudo chown -R  leiline spark/</span><br></pre></td></tr></table></figure>

<ul>
<li>将修改好的spark打包发送到slave服务器中</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">leiline@master:/usr/local/spark$ tar -czvf  spark.tar.gz spark-2.1.1-bin-hadoop2.7/</span><br><span class="line">leiline@master:/usr/local/spark$ scp spark.tar.gz node3:/usr/local/spark</span><br><span class="line">leiline@master:/usr/local/spark$ scp spark.tar.gz node3:/usr/local/spark</span><br></pre></td></tr></table></figure>

<ul>
<li><p>配置master免密登录node1和node3</p>
</li>
<li><p>启动spark集群</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">leiline@master:/usr/local/spark/spark-2.1.1-bin-hadoop2.7/sbin$ sh start-all.sh</span><br></pre></td></tr></table></figure>

<h1 id="向spark-集群中提交jar包"><a href="#向spark-集群中提交jar包" class="headerlink" title="向spark 集群中提交jar包"></a>向spark 集群中提交jar包</h1><p>spark是由scala语言编写的，因此它支持<a href="https://www.scala-lang.org/" target="_blank" rel="noopener">scala</a>编程。spark在编程的时候需要依赖第三方包，这些包主要存储在/spark-2.1.1-bin-hadoop2.7/jars目录下，当然也可以使用sbt或者Maven的形式导入依赖包。</p>
<h2 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h2><p>我们首先编写scala代码，并将代码打成jar包。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main.scala.examples.<span class="type">BdAnalysis</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDs</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="comment">// initializing spark</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"RDDs"</span>).setMaster(<span class="string">"spark://122.204.142.195:7077"</span>).set(<span class="string">"spark.port.maxRetries"</span>, <span class="string">"100"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines = sc.textFile(<span class="string">"file:///usr/local/spark/spark-2.1.1-bin-hadoop2.7/README.md"</span>)</span><br><span class="line">    <span class="keyword">val</span> pythonLines = lines.filter(x =&gt; x.contains(<span class="string">"Python"</span>))</span><br><span class="line">    println(pythonLines.count())</span><br><span class="line">    println(pythonLines.first())</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>代码的功能是首先初始化spark环境配置sc,之后读取文件READE.md，并对每一行进行扫描查看这一行中是否包含字符串”Python”，最后，输出扫描到行数以及第一行的内容。</p>
<p>在idea中打成jar包，得到helloSpark.jar</p>
<h2 id="提交jar包"><a href="#提交jar包" class="headerlink" title="提交jar包"></a>提交jar包</h2><p>spark提交jar包的入口在/spark-2.1.1-bin-hadoop2.7/bin/spark-submit，提交的格式为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... # other options</span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure>

<p>其中有多重选修，–class表示mian方法所在的类名，–master指向master的地址，-deploy-mode表示集群配置方式（分布式或者伪分布式），–conf表示Spark配置属性</p>
<p>详细见<a href="https://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/submitting-applications.html</a></p>
<p>helloSpark.jar提交的格式为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/spark/spark-2.1.1-bin-hadoop2.7/bin/spark-submit --class main.scala.examples.BdAnalysis.RDDs --master spark://122.204.142.195:7077 helloSpark.jar</span><br></pre></td></tr></table></figure>

<p>之后会输出spark执行的日志，其中包括</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">18/03/23 15:05:22 INFO scheduler.DAGScheduler: Job 0 finished: count at RDDs.scala:14, took 5.876258 s</span><br><span class="line">3</span><br><span class="line"></span><br><span class="line">18/03/23 15:05:23 INFO scheduler.DAGScheduler: Job 1 finished: first at RDDs.scala:15, took 0.487181 s</span><br><span class="line">high-level APIs in Scala, Java, Python, and R, and an optimized engine that</span><br><span class="line"></span><br><span class="line">18/03/23 15:05:23 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors</span><br><span class="line">18/03/23 15:05:23 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down</span><br><span class="line">18/03/23 15:05:24 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line">18/03/23 15:05:24 INFO memory.MemoryStore: MemoryStore cleared</span><br><span class="line">18/03/23 15:05:24 INFO storage.BlockManager: BlockManager stopped</span><br><span class="line">18/03/23 15:05:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped</span><br><span class="line">18/03/23 15:05:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class="line">18/03/23 15:05:24 INFO spark.SparkContext: Successfully stopped SparkContext</span><br><span class="line">18/03/23 15:05:24 INFO util.ShutdownHookManager: Shutdown hook called</span><br><span class="line">18/03/23 15:05:24 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c042b23e-8db6-4366-bd57-0456e6e92045</span><br></pre></td></tr></table></figure>

<p>如果日志中没有报错数据，那么就表示成功执行了。</p>
<p>在任务执行的时候，可以打开浏览器输入</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://[master-url]:4040</span><br></pre></td></tr></table></figure>

<p>就可以打开spark ui界面，看到spark任务执行过程中系统资源调用情况，以及任务执行的阶段。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">leiline</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/03/23/install-spark-cluster-and-submit-jar-in-spark-cluster/">http://yoursite.com/2018/03/23/install-spark-cluster-and-submit-jar-in-spark-cluster/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/spark/">spark</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/03/24/spark-input-and-output/"><i class="fa fa-chevron-left">  </i><span>spark数据的读取与保存</span></a></div><div class="next-post pull-right"><a href="/2018/03/22/submit-mapreduce-in-hadoop-cluster/"><span>hadoop集群提交代码</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://pic4.zhimg.com/80/v2-4ec42922163b93f676bba3a169225154_1440w.jpg?source=1940ef5c)"><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2021 By leiline</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>